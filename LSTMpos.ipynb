{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./datasets/DfCleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model n-gram2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "tokenizer = Tokenizer(num_words= max_features, split= ' ')\n",
    "tokenizer.fit_on_texts(df['ngram2'].values)\n",
    "x = tokenizer.texts_to_sequences(df['ngram2'].values)\n",
    "x = pad_sequences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(df['clarity']).values\n",
    "#y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# num_folds = 5\n",
    "# kfold = KFold(n_splits=num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelLSTM():\n",
    "    max_features = 1000\n",
    "    embed_dim = 32\n",
    "    lstm_out = 32\n",
    "    \n",
    "    model = Sequential([\n",
    "        layers.Embedding(max_features, embed_dim, input_length= x.shape[1]),\n",
    "        layers.Conv1D(filters=32, kernel_size=3, padding='same', activation= 'relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.LSTM(128, dropout=0.1, recurrent_dropout=0.1),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KerasClassifier(build_fn=modelLSTM, epochs=6, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chocomenta/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 12:48:31.257023: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 53552000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 174s 1s/step - loss: 0.6323 - accuracy: 0.3785\n",
      "Epoch 2/2\n",
      "125/125 [==============================] - 165s 1s/step - loss: 0.5818 - accuracy: 0.4895\n",
      "63/63 [==============================] - 16s 245ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chocomenta/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 12:54:25.914228: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 53552000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 169s 1s/step - loss: 0.6354 - accuracy: 0.3753\n",
      "Epoch 2/2\n",
      "125/125 [==============================] - 176s 1s/step - loss: 0.5799 - accuracy: 0.5015\n",
      "63/63 [==============================] - 16s 254ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chocomenta/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 13:00:27.019879: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 53552000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 180s 1s/step - loss: 0.6391 - accuracy: 0.3450\n",
      "Epoch 2/2\n",
      "125/125 [==============================] - 178s 1s/step - loss: 0.5935 - accuracy: 0.4680\n",
      "63/63 [==============================] - 16s 244ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.4775, 0.391 , 0.3805])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = cross_val_score(clf, x, y, scoring='accuracy', cv=2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4163333333333334"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model n-gram3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "tokenizer = Tokenizer(num_words= max_features, split= ' ')\n",
    "tokenizer.fit_on_texts(df['ngram3'].values)\n",
    "x = tokenizer.texts_to_sequences(df['ngram3'].values)\n",
    "x = pad_sequences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 5015, 32)          32000     \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 5015, 32)          3104      \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPoolin  (None, 2507, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 128)               82432     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117665 (459.63 KB)\n",
      "Trainable params: 117665 (459.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32\n",
    "lstm_out = 32\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Embedding(max_features, embed_dim, input_length= x.shape[1]),\n",
    "    layers.Conv1D(filters=32, kernel_size=3, padding='same', activation= 'relu'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.LSTM(128, dropout=0.1, recurrent_dropout=0.1),\n",
    "    layers.Dense(1, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kfold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/chocomenta/Desktop/git/LSTMpos.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chocomenta/Desktop/git/LSTMpos.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chocomenta/Desktop/git/LSTMpos.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Itera sobre os folds\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/chocomenta/Desktop/git/LSTMpos.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m kfold\u001b[39m.\u001b[39msplit(x):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chocomenta/Desktop/git/LSTMpos.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     xtrain, xtest \u001b[39m=\u001b[39m x[train_index], x[test_index]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chocomenta/Desktop/git/LSTMpos.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     ytrain, ytest \u001b[39m=\u001b[39m y[train_index], y[test_index]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kfold' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Itera sobre os folds\n",
    "for train_index, test_index in kfold.split(x):\n",
    "    xtrain, xtest = x[train_index], x[test_index]\n",
    "    ytrain, ytest = y[train_index], y[test_index]\n",
    "\n",
    "    history = model.fit(xtrain, ytrain, epochs=3, batch_size=32, verbose=1, validation_data=(xtrain, ytrain), callbacks=[early_stopping])\n",
    "\n",
    "    # Avalie o modelo no conjunto de teste para o fold atual\n",
    "    loss, accuracy = model.evaluate(xtest, ytest, verbose=0, batch_size=32)\n",
    "    results.append(accuracy)\n",
    "\n",
    "# Calcula a média das métricas de avaliação\n",
    "mean_accuracy = np.mean(results)\n",
    "print(\"Mean cross-validated accuracy:\", mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv('./datasets/DfPos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "tokenizer = Tokenizer(num_words= max_features, split= ' ')\n",
    "tokenizer.fit_on_texts(df_pos['pos'].values)\n",
    "x = tokenizer.texts_to_sequences(df_pos['pos'].values)\n",
    "x = pad_sequences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelLSTM():\n",
    "    max_features = 1000\n",
    "    embed_dim = 32\n",
    "    lstm_out = 32\n",
    "    \n",
    "    model = Sequential([\n",
    "        layers.Embedding(max_features, embed_dim, input_length= x.shape[1]),\n",
    "        layers.Conv1D(filters=32, kernel_size=3, padding='same', activation= 'relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.LSTM(128, dropout=0.1, recurrent_dropout=0.1),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KerasClassifier(build_fn=modelLSTM, epochs=6, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chocomenta/anaconda3/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 13:10:43.637038: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 43944000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/94 [===============>..............] - ETA: 1:12 - loss: 0.6390 - accuracy: 0.3672"
     ]
    }
   ],
   "source": [
    "result = cross_val_score(clf, x, y, scoring='accuracy', cv=2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
