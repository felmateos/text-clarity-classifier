{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 23:53:01.442609: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-11 23:53:01.685707: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-11 23:53:01.685743: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-11 23:53:01.686677: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-11 23:53:01.790222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de GPUs disponíveis: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 23:53:05.043440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-11 23:53:05.066459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-11 23:53:05.066508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Número de GPUs disponíveis:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 777\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('../data/BERT_Embeddings_1.csv')\n",
    "df_2 = pd.read_csv('../data/BERT_Embeddings_2.csv')\n",
    "\n",
    "df = pd.concat([df_1,df_2])\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "embeddings = df.iloc[:,8:].values\n",
    "labels = df['Class']\n",
    "\n",
    "# Supondo que você tenha uma coluna 'rotulo' em seu DataFrame\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Convertendo para tensores do PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Criando datasets e dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Defina os parâmetros do modelo\n",
    "input_size = 768  # número de embeddings\n",
    "hidden_size = 16  # tamanho do estado oculto\n",
    "output_size = 3  # substitua num_classes pelo número real de classes\n",
    "\n",
    "# Crie uma instância do modelo\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época [1/500], Perda: 1.1086\n",
      "Época [2/500], Perda: 1.1003\n",
      "Época [3/500], Perda: 1.0985\n",
      "Época [4/500], Perda: 1.0962\n",
      "Época [5/500], Perda: 1.0964\n",
      "Época [6/500], Perda: 1.0943\n",
      "Época [7/500], Perda: 1.0949\n",
      "Época [8/500], Perda: 1.0948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época [9/500], Perda: 1.0916\n",
      "Época [10/500], Perda: 1.0914\n",
      "Época [11/500], Perda: 1.0891\n",
      "Época [12/500], Perda: 1.0882\n",
      "Época [13/500], Perda: 1.0876\n",
      "Época [14/500], Perda: 1.0876\n",
      "Época [15/500], Perda: 1.0846\n",
      "Época [16/500], Perda: 1.0865\n",
      "Época [17/500], Perda: 1.0869\n",
      "Época [18/500], Perda: 1.0831\n",
      "Época [19/500], Perda: 1.0819\n",
      "Época [20/500], Perda: 1.0822\n",
      "Época [21/500], Perda: 1.0799\n",
      "Época [22/500], Perda: 1.0794\n",
      "Época [23/500], Perda: 1.0783\n",
      "Época [24/500], Perda: 1.0771\n",
      "Época [25/500], Perda: 1.0719\n",
      "Época [26/500], Perda: 1.0767\n",
      "Época [27/500], Perda: 1.0773\n",
      "Época [28/500], Perda: 1.0750\n",
      "Época [29/500], Perda: 1.0736\n",
      "Época [30/500], Perda: 1.0659\n",
      "Época [31/500], Perda: 1.0710\n",
      "Época [32/500], Perda: 1.0676\n",
      "Época [33/500], Perda: 1.0676\n",
      "Época [34/500], Perda: 1.0678\n",
      "Época [35/500], Perda: 1.0679\n",
      "Época [36/500], Perda: 1.0658\n",
      "Época [37/500], Perda: 1.0667\n",
      "Época [38/500], Perda: 1.0630\n",
      "Época [39/500], Perda: 1.0629\n",
      "Época [40/500], Perda: 1.0643\n",
      "Época [41/500], Perda: 1.0595\n",
      "Época [42/500], Perda: 1.0597\n",
      "Época [43/500], Perda: 1.0583\n",
      "Época [44/500], Perda: 1.0538\n",
      "Época [45/500], Perda: 1.0535\n",
      "Época [46/500], Perda: 1.0534\n",
      "Época [47/500], Perda: 1.0581\n",
      "Época [48/500], Perda: 1.0570\n",
      "Época [49/500], Perda: 1.0558\n",
      "Época [50/500], Perda: 1.0578\n",
      "Época [51/500], Perda: 1.0582\n",
      "Época [52/500], Perda: 1.0554\n",
      "Época [53/500], Perda: 1.0520\n",
      "Época [54/500], Perda: 1.0519\n",
      "Época [55/500], Perda: 1.0534\n",
      "Época [56/500], Perda: 1.0512\n",
      "Época [57/500], Perda: 1.0530\n",
      "Época [58/500], Perda: 1.0458\n",
      "Época [59/500], Perda: 1.0463\n",
      "Época [60/500], Perda: 1.0481\n",
      "Época [61/500], Perda: 1.0482\n",
      "Época [62/500], Perda: 1.0466\n",
      "Época [63/500], Perda: 1.0507\n",
      "Época [64/500], Perda: 1.0469\n",
      "Época [65/500], Perda: 1.0452\n",
      "Época [66/500], Perda: 1.0500\n",
      "Época [67/500], Perda: 1.0446\n",
      "Época [68/500], Perda: 1.0448\n",
      "Época [69/500], Perda: 1.0408\n",
      "Época [70/500], Perda: 1.0417\n",
      "Época [71/500], Perda: 1.0442\n",
      "Época [72/500], Perda: 1.0402\n",
      "Época [73/500], Perda: 1.0416\n",
      "Época [74/500], Perda: 1.0400\n",
      "Época [75/500], Perda: 1.0432\n",
      "Época [76/500], Perda: 1.0405\n",
      "Época [77/500], Perda: 1.0377\n",
      "Época [78/500], Perda: 1.0375\n",
      "Época [79/500], Perda: 1.0361\n",
      "Época [80/500], Perda: 1.0371\n",
      "Época [81/500], Perda: 1.0359\n",
      "Época [82/500], Perda: 1.0353\n",
      "Época [83/500], Perda: 1.0355\n",
      "Época [84/500], Perda: 1.0390\n",
      "Época [85/500], Perda: 1.0363\n",
      "Época [86/500], Perda: 1.0331\n",
      "Época [87/500], Perda: 1.0333\n",
      "Época [88/500], Perda: 1.0349\n",
      "Época [89/500], Perda: 1.0349\n",
      "Época [90/500], Perda: 1.0313\n",
      "Época [91/500], Perda: 1.0278\n",
      "Época [92/500], Perda: 1.0320\n",
      "Época [93/500], Perda: 1.0313\n",
      "Época [94/500], Perda: 1.0332\n",
      "Época [95/500], Perda: 1.0331\n",
      "Época [96/500], Perda: 1.0357\n",
      "Época [97/500], Perda: 1.0303\n",
      "Época [98/500], Perda: 1.0278\n",
      "Época [99/500], Perda: 1.0248\n",
      "Época [100/500], Perda: 1.0291\n",
      "Época [101/500], Perda: 1.0289\n",
      "Época [102/500], Perda: 1.0343\n",
      "Época [103/500], Perda: 1.0251\n",
      "Época [104/500], Perda: 1.0271\n",
      "Época [105/500], Perda: 1.0256\n",
      "Época [106/500], Perda: 1.0304\n",
      "Época [107/500], Perda: 1.0271\n",
      "Época [108/500], Perda: 1.0225\n",
      "Época [109/500], Perda: 1.0258\n",
      "Época [110/500], Perda: 1.0260\n",
      "Época [111/500], Perda: 1.0194\n",
      "Época [112/500], Perda: 1.0246\n",
      "Época [113/500], Perda: 1.0221\n",
      "Época [114/500], Perda: 1.0183\n",
      "Época [115/500], Perda: 1.0208\n",
      "Época [116/500], Perda: 1.0255\n",
      "Época [117/500], Perda: 1.0203\n",
      "Época [118/500], Perda: 1.0188\n",
      "Época [119/500], Perda: 1.0192\n",
      "Época [120/500], Perda: 1.0222\n",
      "Época [121/500], Perda: 1.0227\n",
      "Época [122/500], Perda: 1.0202\n",
      "Época [123/500], Perda: 1.0214\n",
      "Época [124/500], Perda: 1.0173\n",
      "Época [125/500], Perda: 1.0224\n",
      "Época [126/500], Perda: 1.0158\n",
      "Época [127/500], Perda: 1.0138\n",
      "Época [128/500], Perda: 1.0167\n",
      "Época [129/500], Perda: 1.0169\n",
      "Época [130/500], Perda: 1.0108\n",
      "Época [131/500], Perda: 1.0205\n",
      "Época [132/500], Perda: 1.0171\n",
      "Época [133/500], Perda: 1.0141\n",
      "Época [134/500], Perda: 1.0149\n",
      "Época [135/500], Perda: 1.0130\n",
      "Época [136/500], Perda: 1.0154\n",
      "Época [137/500], Perda: 1.0139\n",
      "Época [138/500], Perda: 1.0170\n",
      "Época [139/500], Perda: 1.0132\n",
      "Época [140/500], Perda: 1.0172\n",
      "Época [141/500], Perda: 1.0167\n",
      "Época [142/500], Perda: 1.0142\n",
      "Época [143/500], Perda: 1.0170\n",
      "Época [144/500], Perda: 1.0126\n",
      "Época [145/500], Perda: 1.0147\n",
      "Época [146/500], Perda: 1.0160\n",
      "Época [147/500], Perda: 1.0098\n",
      "Época [148/500], Perda: 1.0142\n",
      "Época [149/500], Perda: 1.0091\n",
      "Época [150/500], Perda: 1.0083\n",
      "Época [151/500], Perda: 1.0092\n",
      "Época [152/500], Perda: 1.0114\n",
      "Época [153/500], Perda: 1.0119\n",
      "Época [154/500], Perda: 1.0086\n",
      "Época [155/500], Perda: 1.0057\n",
      "Época [156/500], Perda: 1.0068\n",
      "Época [157/500], Perda: 1.0071\n",
      "Época [158/500], Perda: 1.0074\n",
      "Época [159/500], Perda: 1.0093\n",
      "Época [160/500], Perda: 1.0129\n",
      "Época [161/500], Perda: 1.0036\n",
      "Época [162/500], Perda: 1.0073\n",
      "Época [163/500], Perda: 1.0102\n",
      "Época [164/500], Perda: 1.0059\n",
      "Época [165/500], Perda: 1.0042\n",
      "Época [166/500], Perda: 1.0110\n",
      "Época [167/500], Perda: 1.0090\n",
      "Época [168/500], Perda: 1.0038\n",
      "Época [169/500], Perda: 1.0056\n",
      "Época [170/500], Perda: 1.0080\n",
      "Época [171/500], Perda: 1.0067\n",
      "Época [172/500], Perda: 1.0039\n",
      "Época [173/500], Perda: 1.0013\n",
      "Época [174/500], Perda: 1.0053\n",
      "Época [175/500], Perda: 0.9991\n",
      "Época [176/500], Perda: 0.9998\n",
      "Época [177/500], Perda: 1.0028\n",
      "Época [178/500], Perda: 1.0044\n",
      "Época [179/500], Perda: 1.0053\n",
      "Época [180/500], Perda: 1.0037\n",
      "Época [181/500], Perda: 1.0024\n",
      "Época [182/500], Perda: 1.0014\n",
      "Época [183/500], Perda: 1.0034\n",
      "Época [184/500], Perda: 1.0001\n",
      "Época [185/500], Perda: 1.0011\n",
      "Época [186/500], Perda: 0.9971\n",
      "Época [187/500], Perda: 1.0044\n",
      "Época [188/500], Perda: 0.9975\n",
      "Época [189/500], Perda: 1.0008\n",
      "Época [190/500], Perda: 1.0025\n",
      "Época [191/500], Perda: 0.9972\n",
      "Época [192/500], Perda: 0.9960\n",
      "Época [193/500], Perda: 0.9978\n",
      "Época [194/500], Perda: 0.9959\n",
      "Época [195/500], Perda: 1.0003\n",
      "Época [196/500], Perda: 1.0026\n",
      "Época [197/500], Perda: 0.9984\n",
      "Época [198/500], Perda: 1.0007\n",
      "Época [199/500], Perda: 0.9978\n",
      "Época [200/500], Perda: 0.9983\n",
      "Época [201/500], Perda: 0.9992\n",
      "Época [202/500], Perda: 0.9954\n",
      "Época [203/500], Perda: 0.9965\n",
      "Época [204/500], Perda: 0.9969\n",
      "Época [205/500], Perda: 0.9948\n",
      "Época [206/500], Perda: 0.9945\n",
      "Época [207/500], Perda: 0.9944\n",
      "Época [208/500], Perda: 0.9948\n",
      "Época [209/500], Perda: 0.9981\n",
      "Época [210/500], Perda: 0.9957\n",
      "Época [211/500], Perda: 0.9964\n",
      "Época [212/500], Perda: 0.9974\n",
      "Época [213/500], Perda: 0.9944\n",
      "Época [214/500], Perda: 0.9897\n",
      "Época [215/500], Perda: 0.9989\n",
      "Época [216/500], Perda: 0.9880\n",
      "Época [217/500], Perda: 0.9896\n",
      "Época [218/500], Perda: 0.9924\n",
      "Época [219/500], Perda: 0.9908\n",
      "Época [220/500], Perda: 0.9927\n",
      "Época [221/500], Perda: 0.9912\n",
      "Época [222/500], Perda: 0.9915\n",
      "Época [223/500], Perda: 0.9926\n",
      "Época [224/500], Perda: 0.9932\n",
      "Época [225/500], Perda: 0.9909\n",
      "Época [226/500], Perda: 0.9886\n",
      "Época [227/500], Perda: 0.9922\n",
      "Época [228/500], Perda: 0.9875\n",
      "Época [229/500], Perda: 0.9877\n",
      "Época [230/500], Perda: 0.9899\n",
      "Época [231/500], Perda: 0.9906\n",
      "Época [232/500], Perda: 0.9876\n",
      "Época [233/500], Perda: 0.9877\n",
      "Época [234/500], Perda: 0.9891\n",
      "Época [235/500], Perda: 0.9847\n",
      "Época [236/500], Perda: 0.9880\n",
      "Época [237/500], Perda: 0.9864\n",
      "Época [238/500], Perda: 0.9882\n",
      "Época [239/500], Perda: 0.9851\n",
      "Época [240/500], Perda: 0.9848\n",
      "Época [241/500], Perda: 0.9897\n",
      "Época [242/500], Perda: 0.9900\n",
      "Época [243/500], Perda: 0.9856\n",
      "Época [244/500], Perda: 0.9816\n",
      "Época [245/500], Perda: 0.9855\n",
      "Época [246/500], Perda: 0.9854\n",
      "Época [247/500], Perda: 0.9851\n",
      "Época [248/500], Perda: 0.9808\n",
      "Época [249/500], Perda: 0.9876\n",
      "Época [250/500], Perda: 0.9790\n",
      "Época [251/500], Perda: 0.9772\n",
      "Época [252/500], Perda: 0.9890\n",
      "Época [253/500], Perda: 0.9844\n",
      "Época [254/500], Perda: 0.9789\n",
      "Época [255/500], Perda: 0.9851\n",
      "Época [256/500], Perda: 0.9864\n",
      "Época [257/500], Perda: 0.9865\n",
      "Época [258/500], Perda: 0.9829\n",
      "Época [259/500], Perda: 0.9840\n",
      "Época [260/500], Perda: 0.9803\n",
      "Época [261/500], Perda: 0.9818\n",
      "Época [262/500], Perda: 0.9831\n",
      "Época [263/500], Perda: 0.9780\n",
      "Época [264/500], Perda: 0.9848\n",
      "Época [265/500], Perda: 0.9813\n",
      "Época [266/500], Perda: 0.9856\n",
      "Época [267/500], Perda: 0.9790\n",
      "Época [268/500], Perda: 0.9771\n",
      "Época [269/500], Perda: 0.9841\n",
      "Época [270/500], Perda: 0.9793\n",
      "Época [271/500], Perda: 0.9829\n",
      "Época [272/500], Perda: 0.9789\n",
      "Época [273/500], Perda: 0.9798\n",
      "Época [274/500], Perda: 0.9807\n",
      "Época [275/500], Perda: 0.9770\n",
      "Época [276/500], Perda: 0.9798\n",
      "Época [277/500], Perda: 0.9788\n",
      "Época [278/500], Perda: 0.9828\n",
      "Época [279/500], Perda: 0.9806\n",
      "Época [280/500], Perda: 0.9791\n",
      "Época [281/500], Perda: 0.9749\n",
      "Época [282/500], Perda: 0.9773\n",
      "Época [283/500], Perda: 0.9756\n",
      "Época [284/500], Perda: 0.9737\n",
      "Época [285/500], Perda: 0.9765\n",
      "Época [286/500], Perda: 0.9744\n",
      "Época [287/500], Perda: 0.9779\n",
      "Época [288/500], Perda: 0.9797\n",
      "Época [289/500], Perda: 0.9740\n",
      "Época [290/500], Perda: 0.9803\n",
      "Época [291/500], Perda: 0.9810\n",
      "Época [292/500], Perda: 0.9762\n",
      "Época [293/500], Perda: 0.9753\n",
      "Época [294/500], Perda: 0.9788\n",
      "Época [295/500], Perda: 0.9727\n",
      "Época [296/500], Perda: 0.9705\n",
      "Época [297/500], Perda: 0.9732\n",
      "Época [298/500], Perda: 0.9763\n",
      "Época [299/500], Perda: 0.9786\n",
      "Época [300/500], Perda: 0.9770\n",
      "Época [301/500], Perda: 0.9725\n",
      "Época [302/500], Perda: 0.9745\n",
      "Época [303/500], Perda: 0.9723\n",
      "Época [304/500], Perda: 0.9727\n",
      "Época [305/500], Perda: 0.9725\n",
      "Época [306/500], Perda: 0.9727\n",
      "Época [307/500], Perda: 0.9691\n",
      "Época [308/500], Perda: 0.9636\n",
      "Época [309/500], Perda: 0.9677\n",
      "Época [310/500], Perda: 0.9736\n",
      "Época [311/500], Perda: 0.9739\n",
      "Época [312/500], Perda: 0.9675\n",
      "Época [313/500], Perda: 0.9711\n",
      "Época [314/500], Perda: 0.9692\n",
      "Época [315/500], Perda: 0.9651\n",
      "Época [316/500], Perda: 0.9722\n",
      "Época [317/500], Perda: 0.9686\n",
      "Época [318/500], Perda: 0.9671\n",
      "Época [319/500], Perda: 0.9671\n",
      "Época [320/500], Perda: 0.9659\n",
      "Época [321/500], Perda: 0.9666\n",
      "Época [322/500], Perda: 0.9673\n",
      "Época [323/500], Perda: 0.9671\n",
      "Época [324/500], Perda: 0.9697\n",
      "Época [325/500], Perda: 0.9679\n",
      "Época [326/500], Perda: 0.9660\n",
      "Época [327/500], Perda: 0.9735\n",
      "Época [328/500], Perda: 0.9701\n",
      "Época [329/500], Perda: 0.9670\n",
      "Época [330/500], Perda: 0.9635\n",
      "Época [331/500], Perda: 0.9671\n",
      "Época [332/500], Perda: 0.9661\n",
      "Época [333/500], Perda: 0.9624\n",
      "Época [334/500], Perda: 0.9698\n",
      "Época [335/500], Perda: 0.9628\n",
      "Época [336/500], Perda: 0.9676\n",
      "Época [337/500], Perda: 0.9602\n",
      "Época [338/500], Perda: 0.9639\n",
      "Época [339/500], Perda: 0.9711\n",
      "Época [340/500], Perda: 0.9661\n",
      "Época [341/500], Perda: 0.9661\n",
      "Época [342/500], Perda: 0.9678\n",
      "Época [343/500], Perda: 0.9641\n",
      "Época [344/500], Perda: 0.9607\n",
      "Época [345/500], Perda: 0.9576\n",
      "Época [346/500], Perda: 0.9636\n",
      "Época [347/500], Perda: 0.9661\n",
      "Época [348/500], Perda: 0.9629\n",
      "Época [349/500], Perda: 0.9636\n",
      "Época [350/500], Perda: 0.9633\n",
      "Época [351/500], Perda: 0.9663\n",
      "Época [352/500], Perda: 0.9619\n",
      "Época [353/500], Perda: 0.9667\n",
      "Época [354/500], Perda: 0.9604\n",
      "Época [355/500], Perda: 0.9604\n",
      "Época [356/500], Perda: 0.9595\n",
      "Época [357/500], Perda: 0.9644\n",
      "Época [358/500], Perda: 0.9626\n",
      "Época [359/500], Perda: 0.9536\n",
      "Época [360/500], Perda: 0.9630\n",
      "Época [361/500], Perda: 0.9574\n",
      "Época [362/500], Perda: 0.9620\n",
      "Época [363/500], Perda: 0.9646\n",
      "Época [364/500], Perda: 0.9551\n",
      "Época [365/500], Perda: 0.9573\n",
      "Época [366/500], Perda: 0.9578\n",
      "Época [367/500], Perda: 0.9588\n",
      "Época [368/500], Perda: 0.9576\n",
      "Época [369/500], Perda: 0.9567\n",
      "Época [370/500], Perda: 0.9591\n",
      "Época [371/500], Perda: 0.9566\n",
      "Época [372/500], Perda: 0.9621\n",
      "Época [373/500], Perda: 0.9516\n",
      "Época [374/500], Perda: 0.9611\n",
      "Época [375/500], Perda: 0.9574\n",
      "Época [376/500], Perda: 0.9589\n",
      "Época [377/500], Perda: 0.9589\n",
      "Época [378/500], Perda: 0.9508\n",
      "Época [379/500], Perda: 0.9541\n",
      "Época [380/500], Perda: 0.9601\n",
      "Época [381/500], Perda: 0.9602\n",
      "Época [382/500], Perda: 0.9587\n",
      "Época [383/500], Perda: 0.9541\n",
      "Época [384/500], Perda: 0.9571\n",
      "Época [385/500], Perda: 0.9579\n",
      "Época [386/500], Perda: 0.9570\n",
      "Época [387/500], Perda: 0.9529\n",
      "Época [388/500], Perda: 0.9474\n",
      "Época [389/500], Perda: 0.9577\n",
      "Época [390/500], Perda: 0.9574\n",
      "Época [391/500], Perda: 0.9559\n",
      "Época [392/500], Perda: 0.9542\n",
      "Época [393/500], Perda: 0.9557\n",
      "Época [394/500], Perda: 0.9593\n",
      "Época [395/500], Perda: 0.9585\n",
      "Época [396/500], Perda: 0.9569\n",
      "Época [397/500], Perda: 0.9523\n",
      "Época [398/500], Perda: 0.9584\n",
      "Época [399/500], Perda: 0.9532\n",
      "Época [400/500], Perda: 0.9518\n",
      "Época [401/500], Perda: 0.9526\n",
      "Época [402/500], Perda: 0.9519\n",
      "Época [403/500], Perda: 0.9518\n",
      "Época [404/500], Perda: 0.9545\n",
      "Época [405/500], Perda: 0.9552\n",
      "Época [406/500], Perda: 0.9578\n",
      "Época [407/500], Perda: 0.9557\n",
      "Época [408/500], Perda: 0.9586\n",
      "Época [409/500], Perda: 0.9486\n",
      "Época [410/500], Perda: 0.9527\n",
      "Época [411/500], Perda: 0.9568\n",
      "Época [412/500], Perda: 0.9542\n",
      "Época [413/500], Perda: 0.9479\n",
      "Época [414/500], Perda: 0.9519\n",
      "Época [415/500], Perda: 0.9546\n",
      "Época [416/500], Perda: 0.9568\n",
      "Época [417/500], Perda: 0.9490\n",
      "Época [418/500], Perda: 0.9515\n",
      "Época [419/500], Perda: 0.9462\n",
      "Época [420/500], Perda: 0.9462\n",
      "Época [421/500], Perda: 0.9526\n",
      "Época [422/500], Perda: 0.9512\n",
      "Época [423/500], Perda: 0.9487\n",
      "Época [424/500], Perda: 0.9531\n",
      "Época [425/500], Perda: 0.9482\n",
      "Época [426/500], Perda: 0.9449\n",
      "Época [427/500], Perda: 0.9462\n",
      "Época [428/500], Perda: 0.9490\n",
      "Época [429/500], Perda: 0.9504\n",
      "Época [430/500], Perda: 0.9514\n",
      "Época [431/500], Perda: 0.9493\n",
      "Época [432/500], Perda: 0.9481\n",
      "Época [433/500], Perda: 0.9451\n",
      "Época [434/500], Perda: 0.9442\n",
      "Época [435/500], Perda: 0.9452\n",
      "Época [436/500], Perda: 0.9464\n",
      "Época [437/500], Perda: 0.9491\n",
      "Época [438/500], Perda: 0.9488\n",
      "Época [439/500], Perda: 0.9450\n",
      "Época [440/500], Perda: 0.9451\n",
      "Época [441/500], Perda: 0.9436\n",
      "Época [442/500], Perda: 0.9452\n",
      "Época [443/500], Perda: 0.9470\n",
      "Época [444/500], Perda: 0.9402\n",
      "Época [445/500], Perda: 0.9449\n",
      "Época [446/500], Perda: 0.9436\n",
      "Época [447/500], Perda: 0.9491\n",
      "Época [448/500], Perda: 0.9429\n",
      "Época [449/500], Perda: 0.9470\n",
      "Época [450/500], Perda: 0.9444\n",
      "Época [451/500], Perda: 0.9452\n",
      "Época [452/500], Perda: 0.9404\n",
      "Época [453/500], Perda: 0.9475\n",
      "Época [454/500], Perda: 0.9439\n",
      "Época [455/500], Perda: 0.9415\n",
      "Época [456/500], Perda: 0.9528\n",
      "Época [457/500], Perda: 0.9465\n",
      "Época [458/500], Perda: 0.9447\n",
      "Época [459/500], Perda: 0.9432\n",
      "Época [460/500], Perda: 0.9433\n",
      "Época [461/500], Perda: 0.9424\n",
      "Época [462/500], Perda: 0.9456\n",
      "Época [463/500], Perda: 0.9399\n",
      "Época [464/500], Perda: 0.9391\n",
      "Época [465/500], Perda: 0.9428\n",
      "Época [466/500], Perda: 0.9449\n",
      "Época [467/500], Perda: 0.9432\n",
      "Época [468/500], Perda: 0.9462\n",
      "Época [469/500], Perda: 0.9448\n",
      "Época [470/500], Perda: 0.9425\n",
      "Época [471/500], Perda: 0.9436\n",
      "Época [472/500], Perda: 0.9445\n",
      "Época [473/500], Perda: 0.9400\n",
      "Época [474/500], Perda: 0.9412\n",
      "Época [475/500], Perda: 0.9366\n",
      "Época [476/500], Perda: 0.9380\n",
      "Época [477/500], Perda: 0.9383\n",
      "Época [478/500], Perda: 0.9417\n",
      "Época [479/500], Perda: 0.9400\n",
      "Época [480/500], Perda: 0.9405\n",
      "Época [481/500], Perda: 0.9391\n",
      "Época [482/500], Perda: 0.9419\n",
      "Época [483/500], Perda: 0.9383\n",
      "Época [484/500], Perda: 0.9358\n",
      "Época [485/500], Perda: 0.9425\n",
      "Época [486/500], Perda: 0.9361\n",
      "Época [487/500], Perda: 0.9367\n",
      "Época [488/500], Perda: 0.9367\n",
      "Época [489/500], Perda: 0.9453\n",
      "Época [490/500], Perda: 0.9409\n",
      "Época [491/500], Perda: 0.9362\n",
      "Época [492/500], Perda: 0.9384\n",
      "Época [493/500], Perda: 0.9392\n",
      "Época [494/500], Perda: 0.9360\n",
      "Época [495/500], Perda: 0.9450\n",
      "Época [496/500], Perda: 0.9409\n",
      "Época [497/500], Perda: 0.9401\n",
      "Época [498/500], Perda: 0.9372\n",
      "Época [499/500], Perda: 0.9351\n",
      "Época [500/500], Perda: 0.9388\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Defina a função de perda e o otimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Treinamento\n",
    "num_epocas = 500\n",
    "\n",
    "for epoca in range(num_epocas):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    saida = model(X_train_tensor)\n",
    "    perda = criterion(saida, y_train_tensor)\n",
    "    \n",
    "    perda.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Época [{epoca + 1}/{num_epocas}], Perda: {perda.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação no conjunto de teste\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    saida_treino = model(X_train_tensor)\n",
    "    _, predicoes_treino = torch.max(saida_treino, 1)\n",
    "    acuracia_treino = torch.sum(predicoes_treino == y_train_tensor).item() / len(y_train_tensor)\n",
    "\n",
    "    saida_teste = model(X_test_tensor)\n",
    "    _, predicoes_teste = torch.max(saida_teste, 1)\n",
    "    acuracia_teste = torch.sum(predicoes_teste == y_test_tensor).item() / len(y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5647916666666667\n",
      "0.525\n"
     ]
    }
   ],
   "source": [
    "print(acuracia_treino)\n",
    "print(acuracia_teste)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
